# ğŸ§  Local LLM Chatbot with Streamlit & Ollama

## ğŸš€ Description
This project provides a simple Streamlit-based chat interface to interact with a locally hosted LLM using [Ollama](https://ollama.com). It supports streaming responses and maintains chat history using `st.session_state`.
It is a lightweight, interactive chatbot interface built with Streamlit that connects to a locally hosted LLM via Ollama. It allows users to chat with models like gemma3:1b directly on their machine, ensuring privacy, speed, and full control over the model.

## ğŸ› ï¸ Features
- Chat interface with markdown support
- Streaming responses from the model
- Session-based message history
- Easy to switch models (default: `gemma3:1b`)

## ğŸ“¦ Requirements
- Python 3.8+
- Ollama installed and running locally
- Streamlit

## ğŸ”§ Installation

```bash
git clone https://github.com/your-username/LOCAL-LLM-CHATBOT.git
cd LOCAL-LLM-CHATBOT
pip install -r requirements.txt
